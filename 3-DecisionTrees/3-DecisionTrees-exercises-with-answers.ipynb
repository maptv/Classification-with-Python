{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c07a89e",
   "metadata": {},
   "source": [
    "## 3 DECISIONTREES/DECISIONTREES/DECISIONTREES DECISIONTREES 4 EXERCISE ANSWERS ##\n",
    "#### Exercise ####\n",
    "#### Please refer to module 2 of DecisionTrees - DecisionTrees for Tasks 1-6\n",
    "#### Task 1\n",
    "##### Import the required packages\n",
    "##### Set the working directory (home_dir, main_dir, data_dir, plot_dir)\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# import graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "# Set 'main_dir' to location of the project folder\n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n",
    "plot_dir = str(main_dir) + \"/plots\"\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "print(plot_dir)\n",
    "print(plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab76bf",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Read the dataset `heart_failure_clinical_records_dataset.csv` as `ex_df`\n",
    "##### Subset the dataframe to include only numeric and categorical columns\n",
    "##### Delete columns containing either 65% or more than 65% NaN Values\n",
    "##### Write a function to impute NA in both numeric and categorical columns\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "ex_df = pd.read_csv(str(data_dir) + \"/\" + \"heart_failure_clinical_records_dataset.csv\")\n",
    "# Subset data\n",
    "ex_df = ex_df[\n",
    "    [\n",
    "        \"age\",\n",
    "        \"serum_sodium\",\n",
    "        \"time\",\n",
    "        \"platelets\",\n",
    "        \"ejection_fraction\",\n",
    "        \"serum_creatinine\",\n",
    "        \"creatinine_phosphokinase\",\n",
    "        \"anaemia\",\n",
    "        \"high_blood_pressure\",\n",
    "        \"smoking\",\n",
    "        \"sex\",\n",
    "        \"diabetes\",\n",
    "        \"death_event\",\n",
    "        \"id\",\n",
    "    ]\n",
    "]\n",
    "# Delete columns containing either 65% or more than 65% NaN Values\n",
    "ex_perc = 65.0\n",
    "ex_min_count = int(((100 - ex_perc) / 100) * ex_df.shape[0] + 1)\n",
    "ex_df = ex_df.dropna(axis=1, thresh=ex_min_count)\n",
    "\n",
    "\n",
    "# Function to impute NA in both numeric and categorical columns\n",
    "def fillna(ex_df):\n",
    "    # Fill numeric columns with mean value\n",
    "    ex_df = ex_df.fillna(ex_df.mean())\n",
    "    # Fill categorical columns with mode value\n",
    "    ex_df = ex_df.fillna(ex_df.mode().iloc[0])\n",
    "    return ex_df\n",
    "\n",
    "\n",
    "ex_df = fillna(ex_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e2692e",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Identify the the two unique classes\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2db72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the the two unique classes\n",
    "ex_threshold = ex_df[\"death_event\"].mean()\n",
    "ex_df[\"death_event\"] = np.where(ex_df[\"death_event\"] > ex_threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97136474",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### \n",
    "##### Split the data into ex_X and ex_y\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_unique_values = sorted(ex_df[\"death_event\"].unique())\n",
    "ex_df[\"death_event\"] = np.where(\n",
    "    ex_df[\"death_event\"] == ex_unique_values[0], False, True\n",
    ")\n",
    "# Check class again.\n",
    "print(ex_df[\"death_event\"].dtypes)\n",
    "# Split the data into ex_X and ex_y\n",
    "ex_columns_to_drop_from_X = [\"death_event\"] + [\"id\"]\n",
    "ex_X = ex_df.drop(ex_columns_to_drop_from_X, axis=1)\n",
    "ex_y = np.array(ex_df[\"death_event\"])\n",
    "ex_X = pd.get_dummies(\n",
    "    ex_X,\n",
    "    columns=[\"anaemia\", \"high_blood_pressure\", \"smoking\", \"sex\", \"diabetes\"],\n",
    "    dtype=float,\n",
    "    drop_first=True,\n",
    ")\n",
    "print(ex_X.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df084f79",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Implement the Decision Tree and print it.\n",
    "##### Visualize `clf_fit_small`\n",
    "##### Split into train and test.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2d98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Decision Tree on ex_X.\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf_fit = clf.fit(ex_X, ex_y)\n",
    "print(clf_fit)\n",
    "# Visualize `clf_fit_small`\n",
    "tree.plot_tree(clf_fit, feature_names=ex_X.columns, filled=True)\n",
    "plt.show()\n",
    "# Split into train and test.\n",
    "ex_X_train, ex_X_test, ex_y_train, ex_y_test = train_test_split(\n",
    "    ex_X, ex_y, test_size=0.3\n",
    ")\n",
    "print(ex_X_train.shape, ex_y_train.shape)\n",
    "print(ex_X_test.shape, ex_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d8db7",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Implement the Decision Tree on ex_X_train\n",
    "##### Predict on ex_X_test\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437e8f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Decision Tree on ex_X_train.\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf_fit = clf.fit(ex_X_train, ex_y_train)\n",
    "# Predict on ex_X_test.\n",
    "ex_y_predict = clf_fit.predict(ex_X_test)\n",
    "ex_y_predict[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b76bc",
   "metadata": {},
   "source": [
    "#### Please refer to module 3 of DecisionTrees - DecisionTrees for Task 7\n",
    "#### Task 7\n",
    "##### Compute test model accuracy score.\n",
    "##### Compute accuracy using training data.\n",
    "##### Calculate cv scores (accuracy) and print them.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ea2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test model accuracy score.\n",
    "ex_tree_accuracy_score = metrics.accuracy_score(ex_y_test, ex_y_predict)\n",
    "print(\"Accuracy on test data: \", ex_tree_accuracy_score)\n",
    "# Compute accuracy using training data.\n",
    "ex_acc_train_tree = clf_fit.score(ex_X_train, ex_y_train)\n",
    "print(\"Train Accuracy:\", ex_acc_train_tree)\n",
    "ex_clf = tree.DecisionTreeClassifier()\n",
    "ex_cv_scores = cross_val_score(ex_clf, ex_X, ex_y, cv=10)\n",
    "# Print each cv score (accuracy) and average them.\n",
    "print(ex_cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c02fc9",
   "metadata": {},
   "source": [
    "#### Please refer to module 4 of DecisionTrees - DecisionTrees for Tasks 8-12\n",
    "#### Task 8\n",
    "##### Define function that will determine the optimal number for each parameter.\n",
    "##### Optimize: max_depth\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecba455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that will determine the optimal number for each parameter.\n",
    "def optimal_parameter(values, test_results):\n",
    "    best_test_value = max(test_results)\n",
    "    best_test_index = test_results.index(best_test_value)\n",
    "    best_value = values[best_test_index]\n",
    "    return best_value\n",
    "\n",
    "\n",
    "# Fit a Decision Tree with depths ranging from 1 to 32 and plot the training and test accuracy\n",
    "ex_max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "ex_train_results = []\n",
    "ex_test_results = []\n",
    "for max_depth in ex_max_depths:\n",
    "    ex_dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    ex_dt.fit(ex_X_train, ex_y_train)\n",
    "\n",
    "    ex_train_pred = ex_dt.predict(ex_X_train)\n",
    "    ex_acc_train = accuracy_score(ex_y_train, ex_train_pred)\n",
    "\n",
    "    # Add accuracy score to previous train results\n",
    "    ex_train_results.append(ex_acc_train)\n",
    "\n",
    "    ex_y_pred = ex_dt.predict(ex_X_test)\n",
    "    ex_acc_test = accuracy_score(ex_y_test, ex_y_pred)\n",
    "\n",
    "    # Add accuracy score to previous test results\n",
    "    ex_test_results.append(ex_acc_test)\n",
    "# Store optimal max_depth.\n",
    "ex_optimal_max_depth = optimal_parameter(ex_max_depths, ex_test_results)\n",
    "# Plot max depth over 1 - 32.\n",
    "(line1,) = plt.plot(ex_max_depths, ex_train_results, \"b\", label=\"Train accuracy\")\n",
    "(line2,) = plt.plot(ex_max_depths, ex_test_results, \"r\", label=\"Test accuracy\")\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Tree depth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0bdcd",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Optimize: min samples split\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "ex_train_results = []\n",
    "ex_test_results = []\n",
    "for min_samples_split in ex_min_samples_splits:\n",
    "    ex_dt = DecisionTreeClassifier(min_samples_split=min_samples_split)\n",
    "    ex_dt.fit(ex_X_train, ex_y_train)\n",
    "\n",
    "    ex_train_pred = ex_dt.predict(ex_X_train)\n",
    "    ex_acc_train = accuracy_score(ex_y_train, ex_train_pred)\n",
    "\n",
    "    # Add accuracy score to previous train results\n",
    "    ex_train_results.append(ex_acc_train)\n",
    "\n",
    "    ex_y_pred = ex_dt.predict(ex_X_test)\n",
    "    ex_acc_test = accuracy_score(ex_y_test, ex_y_pred)\n",
    "\n",
    "    # Add accuracy score to previous test results\n",
    "    ex_test_results.append(ex_acc_test)\n",
    "# Store optimal max_depth.\n",
    "ex_optimal_min_samples_split = optimal_parameter(ex_min_samples_splits, ex_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a04ba52",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "##### Optimize: Min_samples_leaf \n",
    "##### Optimize: max features\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bebe6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min_samples_leaf:\n",
    "ex_min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "ex_train_results = []\n",
    "ex_test_results = []\n",
    "for min_samples_leaf in ex_min_samples_leafs:\n",
    "    ex_dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n",
    "    ex_dt.fit(ex_X_train, ex_y_train)\n",
    "\n",
    "    ex_train_pred = ex_dt.predict(ex_X_train)\n",
    "    ex_acc_train = accuracy_score(ex_y_train, ex_train_pred)\n",
    "\n",
    "    # Add accuracy score to previous train results\n",
    "    ex_train_results.append(ex_acc_train)\n",
    "\n",
    "    ex_y_pred = ex_dt.predict(ex_X_test)\n",
    "    ex_acc_test = accuracy_score(ex_y_test, ex_y_pred)\n",
    "\n",
    "    # Add accuracy score to previous test results\n",
    "    ex_test_results.append(ex_acc_test)\n",
    "ex_optimal_min_samples_leafs = optimal_parameter(ex_min_samples_leafs, ex_test_results)\n",
    "# Optimize: max features\n",
    "ex_max_features = list(range(1, ex_X.shape[1]))\n",
    "ex_train_results = []\n",
    "ex_test_results = []\n",
    "for max_feature in ex_max_features:\n",
    "    ex_dt = DecisionTreeClassifier(max_features=max_feature)\n",
    "    ex_dt.fit(ex_X_train, ex_y_train)\n",
    "\n",
    "    ex_train_pred = ex_dt.predict(ex_X_train)\n",
    "    ex_acc_train = accuracy_score(ex_y_train, ex_train_pred)\n",
    "\n",
    "    # Add accuracy score to previous train results\n",
    "    ex_train_results.append(ex_acc_train)\n",
    "\n",
    "    ex_y_pred = ex_dt.predict(ex_X_test)\n",
    "    ex_acc_test = accuracy_score(ex_y_test, ex_y_pred)\n",
    "\n",
    "    # Add accuracy score to previous test results\n",
    "    ex_test_results.append(ex_acc_test)\n",
    "ex_optimal_max_features = optimal_parameter(ex_max_features, ex_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1119f",
   "metadata": {},
   "source": [
    "#### Task 11\n",
    "##### Print: ex_optimal_max_depth,ex_optimal_min_samples_split, ex_optimal_min_samples_split, ex_optimal_max_features\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e384b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The optimal max depth is:\", ex_optimal_max_depth)\n",
    "print(\"The optimal min samples split is:\", ex_optimal_min_samples_split)\n",
    "print(\"The optimal min samples leaf is:\", ex_optimal_min_samples_split)\n",
    "print(\"The optimal max features is:\", ex_optimal_max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bbc8f0",
   "metadata": {},
   "source": [
    "#### Task 12\n",
    "##### Set the seed.\n",
    "##### Build Decision Tree classifier with optimized values calculated above\n",
    "##### Compute accuracy of the optimized model using training data\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66bd4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build optimized model\n",
    "np.random.seed(1)\n",
    "# Implement the Decision Tree on ex_X_train.\n",
    "ex_clf_optimized = tree.DecisionTreeClassifier(\n",
    "    max_depth=ex_optimal_max_depth,\n",
    "    min_samples_split=ex_optimal_min_samples_split,\n",
    "    min_samples_leaf=ex_optimal_min_samples_leafs,\n",
    "    max_features=ex_optimal_max_features,\n",
    ")\n",
    "\n",
    "# We can now see our optimized features where before they were just default:\n",
    "print(ex_clf_optimized)\n",
    "ex_clf_optimized_fit = ex_clf_optimized.fit(ex_X_train, ex_y_train)\n",
    "# Predict on ex_X_test.\n",
    "ex_y_predict_optimized = ex_clf_optimized_fit.predict(ex_X_test)\n",
    "# Get the accuracy score.\n",
    "ex_acc_score_tree_optimized = accuracy_score(ex_y_test, ex_y_predict_optimized)\n",
    "print(ex_acc_score_tree_optimized)\n",
    "# Compute accuracy using training data.\n",
    "ex_acc_train_tree_optimized = ex_clf_optimized_fit.score(ex_X_train, ex_y_train)\n",
    "print(\"Train Accuracy:\", ex_acc_train_tree_optimized)"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
