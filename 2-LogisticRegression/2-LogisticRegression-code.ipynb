{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45427ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## 2 LOGISTICREGRESSION/LOGISTICREGRESSION/LOGISTICREGRESSION LOGISTICREGRESSION 2 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344225a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 3: Loading packages  ####\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# Helper packages.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "# Scikit-learn package for logistic regression.\n",
    "from sklearn import linear_model\n",
    "# Model set up and tuning packages from scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Scikit-learn packages for evaluating model performance.\n",
    "from sklearn import metrics\n",
    "# Scikit-learn package for data preprocessing.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 6: Loading data into Python  ####\n",
    "\n",
    "df = pd.read_csv(str(data_dir)+\"/\"+ 'healthcare-dataset-stroke-data.csv')\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 7: Subset data  ####\n",
    "\n",
    "df_subset = df[['age', 'avg_glucose_level', 'heart_disease', 'ever_married', 'hypertension', 'Residence_type', 'gender', 'smoking_status', 'work_type', 'stroke', 'id']]\n",
    "print(df_subset.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5758f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 8: Convert target to binary  ####\n",
    "\n",
    "# Target is binary\n",
    "print(df_subset['stroke'].head())\n",
    "# Identify the the two unique classes\n",
    "unique_values = sorted(df_subset['stroke'].unique())\n",
    "df_subset['stroke'] = np.where(df_subset['stroke'] == unique_values[0],  False,True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 11: Data prep: target variable  ####\n",
    "\n",
    "print(df_subset['stroke'].dtypes)\n",
    "# Identify the the two unique classes\n",
    "unique_values = sorted(df_subset['stroke'].unique())\n",
    "df_subset['stroke'] = np.where(df_subset['stroke'] == unique_values[0],  False,True)\n",
    "# Check class again.\n",
    "print(df_subset['stroke'].dtypes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 12: Data prep: check for NAs  ####\n",
    "\n",
    " # Check for NAs. \n",
    "print(df_subset.isnull().sum())\n",
    "percent_missing = df_subset.isnull().sum() * 100 / len(df_subset)\n",
    "print(percent_missing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f25ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 13: Data prep: check for NAs (cont'd)  ####\n",
    "\n",
    "# Delete columns containing either 50% or more than 50% NaN Values\n",
    "perc = 50.0\n",
    "min_count =  int(((100-perc)/100)*df_subset.shape[0] + 1)\n",
    "df_subset = df_subset.dropna(axis=1, \n",
    "               thresh=min_count)\n",
    "print(df_subset.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47568780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 14: Data prep: check for NAs (cont'd)  ####\n",
    "\n",
    "# Function to impute NA in both numeric and categorical columns\n",
    "def fillna(df):\n",
    "    # Fill numeric columns with mean value\n",
    "    df = df.fillna(df.mean())    \n",
    "    # Fill categorical columns with mode value\n",
    "    df = df.fillna(df.mode().iloc[0])\n",
    "    return df\n",
    "  \n",
    "df_subset = fillna(df_subset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 15: Data prep: split data   ####\n",
    "\n",
    "# Split the data into X and y \n",
    "columns_to_drop_from_X = ['stroke'] + ['id']\n",
    "X = df_subset.drop(columns_to_drop_from_X, axis = 1)\n",
    "y = np.array(df_subset['stroke'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 19: Data prep: convert categorical data columns to dummies  ####\n",
    "\n",
    "print(X.dtypes)\n",
    "X = pd.get_dummies(X, columns = ['heart_disease', 'ever_married', 'hypertension', 'Residence_type', 'gender', 'smoking_status', 'work_type'], dtype=float, drop_first=True)\n",
    "print(X.dtypes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3651462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 20: Split into train and test set  ####\n",
    "\n",
    "# Set the seed.\n",
    "np.random.seed(1)\n",
    "\n",
    "# Split data into train and test sets, use a 70 train - 30 test split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = .3)\n",
    "                                                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 23: Scale the features (cont'd)  ####\n",
    "\n",
    "# Initialize scaler.\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Fit on training data.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Scale training and test data.\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ff62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 27: Logistic regression: build  ####\n",
    "\n",
    "# Set up logistic regression model.\n",
    "logistic_regression_model = linear_model.LogisticRegression()\n",
    "print(logistic_regression_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 29: Logistic regression: fit (cont'd)  ####\n",
    "\n",
    "# Fit the model.\n",
    "logistic_regression_model.fit(X_train_scaled, \n",
    "                              y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94bf66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 30: Logistic regression: predict  ####\n",
    "\n",
    "# Predict on test data.\n",
    "predicted_values = logistic_regression_model.predict(X_test_scaled)\n",
    "print(predicted_values[:20])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfae0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 37: Confusion matrix and accuracy  ####\n",
    "\n",
    "# Take a look at test data confusion matrix.\n",
    "conf_matrix_test = metrics.confusion_matrix(y_test, predicted_values)\n",
    "print(conf_matrix_test)\n",
    "# Compute test model accuracy score.\n",
    "test_accuracy_score = metrics.accuracy_score(y_test, predicted_values)\n",
    "print(\"Accuracy on test data: \", test_accuracy_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730587bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 38: Classification report  ####\n",
    "\n",
    "# Create a list of target names to interpret class assignments.\n",
    "target_names = df_subset['stroke'].unique()\n",
    "target_names=target_names.tolist()\n",
    "target_names = [str(x) for x in target_names]\n",
    "print(class_report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e8c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 42: Save accuracy score  ####\n",
    "\n",
    "model_final = {'metrics' : \"accuracy\" , \n",
    "                'values' : round(test_accuracy_score,4),\n",
    "                'model':'logistic' }\n",
    "print(model_final)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 43: Getting probabilities instead of class labels  ####\n",
    "\n",
    "# Get probabilities instead of predicted values.\n",
    "test_probabilities = logistic_regression_model.predict_proba(X_test_scaled)\n",
    "print(test_probabilities[0:5, :])\n",
    "# Get probabilities of test predictions only.\n",
    "test_predictions = test_probabilities[:, 1]\n",
    "print(test_predictions[0:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f13b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 44: Computing FPR, TPR, and threshold  ####\n",
    "\n",
    "# Get FPR, TPR, and threshold values.\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test,            #<- test data labels\n",
    "                                        test_predictions)  #<- predicted probabilities\n",
    "print(\"False positive: \", fpr[:5])\n",
    "print(\"True positive: \", tpr[:5])\n",
    "print(\"Threshold: \", threshold[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff55b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 45: Computing AUC  ####\n",
    "\n",
    "# Get AUC by providing the FPR and TPR.\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve: \", auc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 46: Putting it all together: ROC plot  ####\n",
    "\n",
    "# Make an ROC curve plot.\n",
    "plt.title('Receiver Operator Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "# Make an ROC curve plot.\n",
    "plt.title('Receiver Operator Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#######################################################\n",
    "####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n",
    "#######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## 2 LOGISTICREGRESSION/LOGISTICREGRESSION/LOGISTICREGRESSION LOGISTICREGRESSION 3 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ec188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 3: Accuracy on train vs. accuracy on test  ####\n",
    "\n",
    "# Compute trained model accuracy score.\n",
    "trained_accuracy_score = logistic_regression_model.score(X_train_scaled, y_train)\n",
    "print(\"Accuracy on train data: \" , trained_accuracy_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 15: Prepare parameters for optimization  ####\n",
    "\n",
    "# Create regularization penalty space.\n",
    "penalty = ['l1', 'l2']\n",
    "# Create regularization constant space.\n",
    "C = np.logspace(0, 10, 10)\n",
    "print(\"Regularization constant: \", C)\n",
    "# Create hyperparameter options dictionary.\n",
    "hyperparameters = dict(C = C, penalty = penalty)\n",
    "print(hyperparameters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35299cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 16: Set up cross-validation logistic function  ####\n",
    "\n",
    "# Grid search 10-fold cross-validation with above parameters.\n",
    "clf = GridSearchCV(linear_model.LogisticRegression(solver='liblinear'), #<- function to optimize\n",
    "                   hyperparameters,                   #<- grid search parameters\n",
    "                   cv = 10,                           #<- 10-fold cv\n",
    "                   verbose = 0)                       #<- no messages to show\n",
    "# Fit CV grid search.\n",
    "best_model = clf.fit(X_train_scaled, y_train)\n",
    "best_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6210ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 17: Check best parameters found by CV  ####\n",
    "\n",
    "# Get best penalty and constant parameters.\n",
    "penalty = best_model.best_estimator_.get_params()['penalty']\n",
    "constant = best_model.best_estimator_.get_params()['C']\n",
    "print('Best penalty: ', penalty)\n",
    "print('Best C: ', constant)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e47a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 18: Predict using the best model parameters  ####\n",
    "\n",
    "# Predict on test data using best model.\n",
    "best_predicted_values = best_model.predict(X_test_scaled)\n",
    "print(best_predicted_values)\n",
    "# Compute best model accuracy score.\n",
    "best_accuracy_score = metrics.accuracy_score(y_test, best_predicted_values)\n",
    "print(\"Accuracy on test data (best model): \", best_accuracy_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc790a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 19: Accuracy on train vs. accuracy on test  ####\n",
    "\n",
    "# Compute trained model accuracy score.\n",
    "trained_accuracy_score = best_model.score(X_train_scaled, y_train)\n",
    "print(\"Accuracy on train data: \" , trained_accuracy_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30930088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 20: Assessing the tuned model  ####\n",
    "\n",
    "# Compute confusion matrix for best model.\n",
    "best_confusion_matrix = metrics.confusion_matrix(y_test, best_predicted_values)\n",
    "print(best_confusion_matrix)\n",
    "# Create a list of target names to interpret class assignments.\n",
    "target_names = ['Low value', 'High value']\n",
    "print(best_class_report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ceafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 21: Save accuracy score  ####\n",
    "\n",
    "model_final = {'metrics' : \"accuracy\", \n",
    "                                  'values' : round(best_accuracy_score, 4),\n",
    "                                  'model':'logistic_tuned' }\n",
    "print(model_final)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 22: Get metrics for ROC curve  ####\n",
    "\n",
    "# Get probabilities instead of predicted values.\n",
    "best_test_probabilities = best_model.predict_proba(X_test_scaled)\n",
    "print(best_test_probabilities[0:5, ])\n",
    "# Get probabilities of test predictions only.\n",
    "best_test_predictions = best_test_probabilities[:, 1]\n",
    "print(best_test_predictions[0:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ec08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 23: Get metrics for ROC curve (cont'd)  ####\n",
    "\n",
    "# Get ROC curve metrics.\n",
    "best_fpr, best_tpr, best_threshold = metrics.roc_curve(y_test, best_test_predictions)\n",
    "best_auc = metrics.auc(best_fpr, best_tpr)\n",
    "print(best_auc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 24: Plot ROC curve for both models  ####\n",
    "\n",
    "# Make an ROC curve plot.\n",
    "plt.title('Receiver Operator Characteristic')\n",
    "plt.plot(fpr, tpr, 'blue', \n",
    "         label = 'AUC = %0.2f'%auc)\n",
    "plt.plot(best_fpr, best_tpr, 'black', \n",
    "         label = 'AUC (best) = %0.2f'%best_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# Make an ROC curve plot.\n",
    "plt.title('Receiver Operator Characteristic')\n",
    "plt.plot(fpr, tpr, 'blue', label = 'AUC = %0.2f' % auc)\n",
    "plt.plot(best_fpr, best_tpr, 'black', label = 'AUC (optimized) = %0.2f' % best_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 26: Exercise  ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n",
    "#######################################################\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
