{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5668e134",
   "metadata": {},
   "source": [
    "## 2 LOGISTICREGRESSION/LOGISTICREGRESSION/LOGISTICREGRESSION LOGISTICREGRESSION 3 EXERCISE ANSWERS ##\n",
    "#### Exercise ####\n",
    "#### Please refer to module 2 of LogisticRegression - LogisticRegression for Tasks 1-8\n",
    "#### Task 1\n",
    "##### Import the required packages. (Revisit this question as you complete the remaining exercises)\n",
    "##### Set the working directory to data directory.\n",
    "##### Print the working directory.\n",
    "##### Import the dataset `heart_failure_clinical_records_dataset.csv` as `ex_df`.\n",
    "##### Subset the DataFrame to include only numeric and categorical columns.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Scikit-learn package for logistic regression.\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Model set up and tuning packages from scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Scikit-learn packages for evaluating model performance.\n",
    "from sklearn import metrics\n",
    "\n",
    "# Scikit-learn package for data preprocessing.\n",
    "from sklearn import preprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "# Set 'main_dir' to location of the project folder.\n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n",
    "# Import the dataset\n",
    "ex_df = pd.read_csv(str(data_dir) + \"/\" + \"heart_failure_clinical_records_dataset.csv\")\n",
    "# Subset data\n",
    "ex_df_subset = ex_df[\n",
    "    [\n",
    "        \"age\",\n",
    "        \"serum_sodium\",\n",
    "        \"time\",\n",
    "        \"platelets\",\n",
    "        \"ejection_fraction\",\n",
    "        \"serum_creatinine\",\n",
    "        \"creatinine_phosphokinase\",\n",
    "        \"anaemia\",\n",
    "        \"high_blood_pressure\",\n",
    "        \"smoking\",\n",
    "        \"sex\",\n",
    "        \"diabetes\",\n",
    "        \"death_event\",\n",
    "        \"id\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f2c37",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### If target is not binary, convert it to binary (Hint: calculate the mean and assign the above mean to 1 and below to 0).\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd8dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target not binary\n",
    "ex_threshold = np.mean(ex_df_subset[\"death_event\"])\n",
    "ex_df_subset[\"death_event\"] = np.where(ex_df_subset[\"death_event\"] > ex_threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786664f",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Identify the two unique classes.\n",
    "##### Convert ex_target to `bool` so that it is a binary class.\n",
    "##### Check and convert NAs.\n",
    "##### Delete columns containing either 50% or more than 50% NaN Values.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69bfc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the two unique classes.\n",
    "unique_values = sorted(ex_df_subset[\"death_event\"].unique())\n",
    "ex_df_subset[\"death_event\"] = np.where(\n",
    "    ex_df_subset[\"death_event\"] == unique_values[0], False, True\n",
    ")\n",
    "# Check for NAs.\n",
    "print(ex_df_subset.isnull().sum())\n",
    "# Delete columns containing either 60% or more than 60% NaN Values.\n",
    "ex_perc = 60.0\n",
    "ex_min_count = int(((100 - ex_perc) / 100) * ex_df_subset.shape[0] + 1)\n",
    "ex_df_subset = ex_df_subset.dropna(axis=1, thresh=ex_min_count)\n",
    "print(ex_df_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a35be",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Write a function to impute NAs in both numeric and categorical columns.\n",
    "##### Split the data into X and y .\n",
    "##### Convert the categorical data to dummy variables.\n",
    "##### Set a seed and split data into train and test sets, use a 70 train - 30 test split.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to impute NA in both numeric and categorical columns\n",
    "def fillna(ex_df):\n",
    "    # Fill numeric columns with mean value\n",
    "    ex_df = ex_df.fillna(ex_df.mean())\n",
    "    # Fill categorical columns with mode value\n",
    "    ex_df = ex_df.fillna(ex_df.mode().iloc[0])\n",
    "    return ex_df\n",
    "\n",
    "\n",
    "ex_df_subset = fillna(ex_df_subset)\n",
    "# Split the data into X and y\n",
    "ex_columns_to_drop_from_X = [\"death_event\"] + [\"id\"]\n",
    "ex_X = ex_df_subset.drop(ex_columns_to_drop_from_X, axis=1)\n",
    "ex_y = np.array(ex_df_subset[\"death_event\"])\n",
    "# Convert categorical variables to dummy variables\n",
    "ex_X = pd.get_dummies(\n",
    "    ex_X,\n",
    "    columns=[\"anaemia\", \"high_blood_pressure\", \"smoking\", \"sex\", \"diabetes\"],\n",
    "    dtype=float,\n",
    "    drop_first=True,\n",
    ")\n",
    "print(ex_X.dtypes)\n",
    "# Set the seed.\n",
    "np.random.seed(1)\n",
    "# Split data into train and test sets, use a 70 train - 30 test split.\n",
    "ex_X_train, ex_X_test, ex_y_train, ex_y_test = train_test_split(\n",
    "    ex_X, ex_y, test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3ec5a",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Scale the features by fitting MinMaxScaler on the training data.\n",
    "##### Set up logistic regression model.\n",
    "##### Fit the model.\n",
    "##### Predict on test data.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features.\n",
    "ex_scaler = preprocessing.MinMaxScaler()\n",
    "ex_scaler.fit(ex_X_train)\n",
    "ex_X_train_scaled = ex_scaler.transform(ex_X_train)\n",
    "ex_X_test_scaled = ex_scaler.transform(ex_X_test)\n",
    "# Set up logistic regression model.\n",
    "ex_logistic_regression_model = linear_model.LogisticRegression()\n",
    "print(ex_logistic_regression_model)\n",
    "# Fit the model.\n",
    "ex_logistic_regression_model.fit(ex_X_train_scaled, ex_y_train)\n",
    "# Predict on test data.\n",
    "ex_predicted_values = ex_logistic_regression_model.predict(ex_X_test_scaled)\n",
    "print(ex_predicted_values[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8aa152",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Take a look at test data confusion matrix.\n",
    "##### Compute test model accuracy score.\n",
    "##### Create a list of ex_target names to interpret class assignments.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49291c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at test data confusion matrix.\n",
    "ex_conf_matrix_test = metrics.confusion_matrix(ex_y_test, ex_predicted_values)\n",
    "print(ex_conf_matrix_test)\n",
    "# Compute test model accuracy score.\n",
    "ex_test_accuracy_score = metrics.accuracy_score(ex_y_test, ex_predicted_values)\n",
    "print(\"Accuracy on test data: \", ex_test_accuracy_score)\n",
    "# Create a list of ex_target names to interpret class assignments.\n",
    "ex_target_names = ex_df_subset[\"death_event\"].unique()\n",
    "ex_target_names = ex_target_names.tolist()\n",
    "ex_target_names = [str(x) for x in ex_target_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caac8b6",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Print an entire classification report.\n",
    "##### Get probabilities instead of predicted values.\n",
    "##### Get probabilities of test predictions only.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92429ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an entire classification report.\n",
    "ex_class_report = metrics.classification_report(\n",
    "    ex_y_test, ex_predicted_values, target_names=ex_target_names\n",
    ")\n",
    "print(ex_class_report)\n",
    "# Get probabilities instead of predicted values.\n",
    "ex_test_probabilities = ex_logistic_regression_model.predict_proba(ex_X_test_scaled)\n",
    "print(ex_test_probabilities[0:5, :])\n",
    "# Get probabilities of test predictions only.\n",
    "ex_test_predictions = ex_test_probabilities[:, 1]\n",
    "print(ex_test_predictions[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410fb9d",
   "metadata": {},
   "source": [
    "#### Task 8\n",
    "##### Get FPR, TPR, and threshold values.\n",
    "##### Get AUC by providing the FPR and TPR.\n",
    "##### Make an ROC curve plot.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get FPR, TPR, and threshold values.\n",
    "ex_fpr, ex_tpr, ex_threshold = metrics.roc_curve(ex_y_test, ex_test_predictions)\n",
    "print(\"False positive: \", ex_fpr[:5])\n",
    "print(\"True positive: \", ex_tpr[:5])\n",
    "print(\"Threshold: \", ex_threshold[:5])\n",
    "# Get AUC by providing the FPR and TPR.\n",
    "ex_auc = metrics.auc(ex_fpr, ex_tpr)\n",
    "print(\"Area under the ROC curve: \", ex_auc)\n",
    "# Make an ROC curve plot.\n",
    "plt.title(\"Receiver Operator Characteristic\")\n",
    "plt.plot(ex_fpr, ex_tpr, \"b\", label=\"AUC = %0.2f\" % ex_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07faa31",
   "metadata": {},
   "source": [
    "#### Please refer to module 3 of LogisticRegression - LogisticRegression for Tasks 9-13\n",
    "#### Task 9\n",
    "##### Compute trained model accuracy score. Complete the code by filling in the None:ex_trained_accuracy_score = logistic_regression_model.score(None, None)\n",
    "##### Create regularization penalty space with l1 and l2.\n",
    "##### Create regularization constant space.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trained model accuracy score.\n",
    "ex_trained_accuracy_score = ex_logistic_regression_model.score(\n",
    "    ex_X_train_scaled, ex_y_train\n",
    ")\n",
    "print(\"Accuracy on train data: \", ex_trained_accuracy_score)\n",
    "# Create regularization penalty space.\n",
    "ex_penalty = [\"l1\", \"l2\"]\n",
    "# Create regularization constant space.\n",
    "ex_C = np.logspace(0, 10, 10)\n",
    "print(\"Regularization constant: \", ex_C)\n",
    "# Create hyperparameter options dictionary.\n",
    "ex_hyperparameters = dict(C=ex_C, penalty=ex_penalty)\n",
    "print(ex_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2970d845",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "##### Execute a grid search 15-fold cross-validation with above parameters.\n",
    "##### Fit CV grid search and print the best_model\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d176a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search 15-fold cross-validation with above parameters.\n",
    "ex_clf = GridSearchCV(\n",
    "    linear_model.LogisticRegression(solver=\"liblinear\"),\n",
    "    ex_hyperparameters,\n",
    "    cv=15,\n",
    "    verbose=0,\n",
    ")\n",
    "# Fit CV grid search.\n",
    "ex_best_model = ex_clf.fit(ex_X_train_scaled, ex_y_train)\n",
    "print(ex_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5ed02",
   "metadata": {},
   "source": [
    "#### Task 11\n",
    "##### Get best penalty and constant parameters.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best penalty and constant parameters.\n",
    "ex_penalty = ex_best_model.best_estimator_.get_params()[\"penalty\"]\n",
    "ex_constant = ex_best_model.best_estimator_.get_params()[\"C\"]\n",
    "print(\"Best penalty: \", ex_penalty)\n",
    "print(\"Best C: \", ex_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f267806c",
   "metadata": {},
   "source": [
    "#### Task 12\n",
    "##### Predict on test data using best model and print it.\n",
    "##### Compute best model accuracy score and print it.\n",
    "##### Compute confusion matrix for best model and print it.\n",
    "##### Create a list of target names to interpret class assignments.\n",
    "##### Compute classification report for best model.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4ec5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data using best model.\n",
    "ex_best_predicted_values = ex_best_model.predict(ex_X_test_scaled)\n",
    "print(ex_best_predicted_values)\n",
    "# Compute best model accuracy score.\n",
    "ex_best_accuracy_score = metrics.accuracy_score(ex_y_test, ex_best_predicted_values)\n",
    "print(\"Accuracy on test data (best model): \", ex_best_accuracy_score)\n",
    "# Compute trained model accuracy score.\n",
    "ex_trained_accuracy_score = ex_best_model.score(ex_X_train_scaled, ex_y_train)\n",
    "print(\"Accuracy on train data: \", ex_trained_accuracy_score)\n",
    "# Compute confusion matrix for best model.\n",
    "ex_best_confusion_matrix = metrics.confusion_matrix(ex_y_test, ex_best_predicted_values)\n",
    "print(ex_best_confusion_matrix)\n",
    "# Create a list of target names to interpret class assignments.\n",
    "ex_target_names = [\"Low value\", \"High value\"]\n",
    "# Compute classification report for best model.\n",
    "ex_best_class_report = metrics.classification_report(\n",
    "    ex_y_test, ex_best_predicted_values, target_names=ex_target_names\n",
    ")\n",
    "print(ex_best_class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f6928",
   "metadata": {},
   "source": [
    "#### Task 13\n",
    "##### Get probabilities instead of predicted values.\n",
    "##### Get probabilities of test predictions only.\n",
    "##### Get ROC curve metrics.\n",
    "##### Make an ROC curve plot.\n",
    "#### Result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d3ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities instead of predicted values.\n",
    "ex_best_test_probabilities = ex_best_model.predict_proba(ex_X_test_scaled)\n",
    "print(ex_best_test_probabilities[0:5,])\n",
    "# Get probabilities of test predictions only.\n",
    "ex_best_test_predictions = ex_best_test_probabilities[:, 1]\n",
    "print(ex_best_test_predictions[0:5])\n",
    "# Get ROC curve metrics.\n",
    "ex_best_fpr, ex_best_tpr, ex_best_threshold = metrics.roc_curve(\n",
    "    ex_y_test, ex_best_test_predictions\n",
    ")\n",
    "ex_best_auc = metrics.auc(ex_best_fpr, ex_best_tpr)\n",
    "print(ex_best_auc)\n",
    "# Make an ROC curve plot.\n",
    "plt.title(\"Receiver Operator Characteristic\")\n",
    "plt.plot(ex_fpr, ex_tpr, \"blue\", label=\"AUC = %0.2f\" % ex_auc)\n",
    "plt.plot(ex_best_fpr, ex_best_tpr, \"black\", label=\"AUC (best) = %0.2f\" % ex_best_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
